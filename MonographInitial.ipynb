{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e0f746-dbd5-41b1-ac83-cf3d93d59e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 15:31:51.740787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-21 15:31:52.231628: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 15:31:52.323301: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-05-21 15:31:52.323323: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-05-21 15:31:54.084380: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-21 15:31:54.085096: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-21 15:31:54.085104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import keras\n",
    "import Levenshtein\n",
    "import pickle\n",
    "import json\n",
    "import zipfile\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np # Import numpy for vector display\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99861562-25ef-4e8e-9382-c634e66f8f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load up to 50000 records...\n",
      "Reached limit of 50000 records.\n",
      "Successfully saved 50000 title-content pairs (out of a maximum of 50000) to 'data/title_content_pair.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# --- MonographInitial.ipynb - Modified Cell 2 ---\n",
    "\n",
    "# <<< MODIFICATION: Define how many records you want to load >>>\n",
    "MAX_RECORDS_TO_LOAD = 50000\n",
    "\n",
    "output_filepath = 'data/title_content_pair.pkl'\n",
    "title_content_list = []\n",
    "records_loaded = 0\n",
    "\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to load up to {MAX_RECORDS_TO_LOAD} records...\")\n",
    "\n",
    "with open('/home/daniel-diaz/Downloads/signalmedia-1m.jsonl/sample-1M.jsonl', 'r') as fp:\n",
    "    for line in fp:\n",
    "        if records_loaded >= MAX_RECORDS_TO_LOAD:\n",
    "            print(f\"Reached limit of {MAX_RECORDS_TO_LOAD} records.\")\n",
    "            break\n",
    "\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                title = data.get('title')\n",
    "                content = data.get('content')\n",
    "                if title and content:\n",
    "                    # --- MODIFICATION: Add START and END tokens to titles HERE ---\n",
    "                    processed_title = '<start> ' + title.strip() + ' <end>'\n",
    "                    processed_content = content.strip() # Content doesn't need start/end\n",
    "\n",
    "                    # Append the processed title and content\n",
    "                    title_content_list.append({'title': processed_title, 'content': processed_content})\n",
    "                    records_loaded += 1\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} on line {records_loaded + 1}\")\n",
    "\n",
    "# Save the list of title-content dictionaries to a pickle file\n",
    "with open(output_filepath, 'wb') as outfile:\n",
    "    pickle.dump(title_content_list, outfile)\n",
    "\n",
    "print(f\"Successfully saved {len(title_content_list)} title-content pairs (out of a maximum of {MAX_RECORDS_TO_LOAD}) to '{output_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801fcb2c-e858-408b-8fe9-c56398557ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> The Return Of The Nike Air Max Sensation Has 80â€™s Babies Hyped! <end>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = 'data/title_content_pair.pkl'\n",
    "with open(filepath, 'rb') as fp:\n",
    "    loaded_data = pickle.load(fp)\n",
    "title_list = []\n",
    "content_list =[]\n",
    "\n",
    "for item in loaded_data:\n",
    "    title = item.get('title')\n",
    "    title_list.append(title)\n",
    "    \n",
    "    content = item.get('content')\n",
    "    content_list.append(content)\n",
    "    # Now you can work with the individual title and content\n",
    "    # For example, you can print them:\n",
    "print(title_list[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e839ac5a-c3b8-4c76-833d-5a8515e4bace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Product Gives Marketers Access to Real Keywords, Conversions and Results Along With 13 Months of Historical Data \\n\\nSAN FRANCISCO, CA -- (Marketwired) -- 09/17/15 -- Jumpshot, a marketing analytics company that uses distinctive data sources to paint a complete picture of the online customer journey, today announced the launch of Jumpshot Elite, giving marketers insight into what their customers are doing the 99% of the time they\\'re not on your site. For years, marketers have been unable to see what organic and paid search terms users were entering, much less tie those searches to purchases. Jumpshot not only injects that user search visibility back into the market, but also makes it possible to tie those keywords to conversions -- for any web site. \\n\\n\"Ever since search engines encrypted search results, marketers have been in the dark about keywords, impacting not only the insight into their own search investments, but also their ability to unearth high converting keywords for their competitors,\" said Deren Baker, CEO of Jumpshot. \"Our platform eliminates the hacks, assumptions, and guesswork that marketers are doing now and provides real data: actual searches tied to actual conversions conducted by real people with nothing inferred.\" \\n\\nUnlike other keyword research tools that receive data through the Adwords API or send bots to cobble together various data inputs and implied metrics, Jumpshot leverages its panel of over 115 million global consumers to analyze real search activity. As a result, Jumpshot is able to provide companies with actionable data to improve the ROI of their search marketing campaigns, SEO tactics and content marketing initiatives. \\n\\nAvailable today, Jumpshot Elite provides 13 months of backward-looking data as well as: \\n\\nAccess to real queries used by searchers \\n\\nPaid and organic results for any website \\n\\nVisibility into organic keywords, eliminating the \"not provided\" outcome in web analytics \\n\\nReal user queries, clicks and transactions instead of machine-generated clicks with inferred results \\n\\nAbility to tie keywords to real transactions on any website \\n\\nVariable attribution models and lookback windows \\n\\nLaunched in January, 2015, Jumpshot grew out of the ambitions of a group of smart marketers and data scientists who were frustrated about the limitations of the data they had access to, and excited about the opportunity to provide new insights into online behavior. \\n\\nThe company uses distinctive data sources to paint a complete picture of the online world for businesses, from where customers spend time online to what they do there and how they get from place to place. By tracking the online customer journey down to each click, Jumpshot reveals how and why customers arrive at purchase decisions. The company tracks more data in more detail than other services, tracking 160 billion monthly clicks generated by its extensive data panel. \\n\\nAbout Jumpshot \\n\\nJumpshot is a marketing analytics platform that reveals the entire customer journey -- from the key sources of traffic to a site, to browsing and buying behavior on any domain. With a panel of 115 million users, Jumpshot provides marketers with the insight to understand what their customers are doing the 99% of the time they\\'re not on their own site -- a scope of information never before attainable. Jumpshot was founded in 2015 and is headquartered in San Francisco. \\n\\nFor more information, please visit www.jumpshot.com. \\n\\nImage Available: http://www2.marketwire.com/mw/frame_mw?attachid=2889222 \\n\\nKelly Mayes \\n\\nThe Bulleit Group \\n\\n615-200-8845 \\n\\nPublished Sep. 17, 2015 \\n\\nCopyright Â© 2015 SYS-CON Media, Inc. â€” All Rights Reserved. \\n\\nSyndicated stories and blog feeds, all rights reserved by the author.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list[5]\n",
    "content_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058b7878-7672-4d62-9de4-f8f6244480ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 825035\n",
      "Vocabulary (first 10): ['<start>', 'Worcester', 'breakfast', 'club', 'for', 'veterans', 'gives', 'hunger', 'its', 'marching']\n",
      "Word Counts (first 10): {'<start>': 50000, 'Worcester': 125, 'breakfast': 370, 'club': 1506, 'for': 194210, 'veterans': 425, 'gives': 1906, 'hunger': 201, 'its': 36517, 'marching': 93}\n"
     ]
    }
   ],
   "source": [
    "# In Cell 5:\n",
    "\n",
    "\n",
    "def get_vocab_and_counts(list_of_texts):\n",
    "    # Consider adding text cleaning (lowercase, remove punctuation) here\n",
    "    word_counts = Counter(word for text in list_of_texts for word in text.split())\n",
    "    vocabulary_list = list(word_counts.keys()) # <<< Return a LIST\n",
    "    return vocabulary_list, word_counts\n",
    "\n",
    "all_texts = title_list + content_list\n",
    "vocabulary, word_counts = get_vocab_and_counts(all_texts) # 'vocabulary' is now a list\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(\"Vocabulary (first 10):\", vocabulary[:10])\n",
    "print(\"Word Counts (first 10):\", dict(list(word_counts.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae64ef8b-67bb-44c5-9fc4-023d60b398df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sentences for Word2Vec from 100000 texts...\n",
      "Created 100000 tokenized sentences.\n",
      "\n",
      "Example tokenized sentences (first 3):\n",
      "['<start>', 'worcester', 'breakfast', 'club', 'for', 'veterans', 'gives', 'hunger', 'its', 'marching', 'orders', '<end>']\n",
      "['<start>', 'jumpshot', 'gives', 'marketers', 'renewed', 'visibility', 'into', 'paid', 'and', 'organic', 'keywords', 'with', 'launch', 'of', 'jumpshot', 'elite', '<end>']\n",
      "['<start>', 'the', 'return', 'of', 'the', 'nike', 'air', 'max', 'sensation', 'has', '80â€™s', 'babies', 'hyped!', '<end>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Preparing sentences for Word2Vec from {len(all_texts)} texts...\")\n",
    "\n",
    "# Tokenize the texts: Convert each text string into a list of words\n",
    "# Simple tokenization: lowercase and split by space.\n",
    "# You might want more sophisticated cleaning/tokenization here.\n",
    "sentences = [text.lower().split() for text in all_texts]\n",
    "\n",
    "print(f\"Created {len(sentences)} tokenized sentences.\")\n",
    "\n",
    "# Optional: Check the first few tokenized sentences\n",
    "print(\"\\nExample tokenized sentences (first 3):\")\n",
    "for i in range(min(3, len(sentences))):\n",
    "    # Print first 20 tokens, or fewer if sentence is short\n",
    "    print(sentences[i][:min(20, len(sentences[i]))])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626f1e81-a3f3-49ee-8ab5-b04d72feee1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ff93df-eff6-448c-9226-3bf9c49ed2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Word2Vec model with vector_size=100, window=5, min_count=5...\n",
      "Word2Vec model training complete.\n",
      "Model saved to 'data/word2vec_model.model'\n"
     ]
    }
   ],
   "source": [
    "#skip this cell\n",
    "# --- In a new cell ---\n",
    "\n",
    "# Define Word2Vec parameters\n",
    "VECTOR_SIZE = 100  # Dimensionality of the word vectors (similar to EMBEDDING_DIM in GloVe)\n",
    "WINDOW = 5         # Max distance between current and predicted word within a sentence\n",
    "MIN_COUNT = 5      # Ignores words with total frequency lower than this\n",
    "WORKERS = os.cpu_count() # Use all available CPU cores for faster training\n",
    "SG = 1             # Training algorithm: 1 for skip-gram; 0 for CBOW. Skip-gram often works better.\n",
    "\n",
    "print(f\"\\nTraining Word2Vec model with vector_size={VECTOR_SIZE}, window={WINDOW}, min_count={MIN_COUNT}...\")\n",
    "\n",
    "# Instantiate and train the model\n",
    "# The training happens automatically when the object is created\n",
    "word2vec_model = Word2Vec(sentences=sentences,\n",
    "                          vector_size=VECTOR_SIZE,\n",
    "                          window=WINDOW,\n",
    "                          min_count=MIN_COUNT,\n",
    "                          workers=WORKERS,\n",
    "                          sg=SG)\n",
    "\n",
    "print(\"Word2Vec model training complete.\")\n",
    "\n",
    "# --- Optional: Save the trained model for later use ---\n",
    "model_save_path = \"data/word2vec_model.model\"\n",
    "os.makedirs('data', exist_ok=True) # Ensure data directory exists\n",
    "word2vec_model.save(model_save_path)\n",
    "print(f\"Model saved to '{model_save_path}'\")\n",
    "\n",
    "# --- To load the model later ---\n",
    "# loaded_model = Word2Vec.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69937c9d-3bbe-4b55-b693-d461742bd94e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model from 'data/word2vec_model.model'...\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Vocabulary Inspection ---\n",
      "Vocabulary Size: 131000\n",
      "First 20 words in vocabulary: ['the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that', 'with', 'at', 'as', 'was', 'by', 'it', 'be', 'are', 'from', 'this']\n",
      "\n",
      "--- Word Vector Inspection ---\n",
      "Vector for the word 'the':\n",
      "  Dimensions: 100\n",
      "  First 10 dimensions: [ 0.0714 -0.2894 -0.0038  0.0561 -0.208  -0.1091 -0.1322  0.1356  0.0454\n",
      " -0.04  ]\n",
      "\n",
      "--- Word Similarity Inspection ---\n",
      "Words most similar to 'train':\n",
      "  - trains (Score: 0.7759)\n",
      "  - train, (Score: 0.7665)\n",
      "  - carriages (Score: 0.7662)\n",
      "  - keleti (Score: 0.7406)\n",
      "  - london-bound (Score: 0.7381)\n"
     ]
    }
   ],
   "source": [
    "#continue from here\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "model_path = \"data/word2vec_model.model\"\n",
    "words_to_inspect = ['train'] # Words to check similarity for\n",
    "num_similar = 5 # How many similar words to show for each test word\n",
    "# --- ---\n",
    "word2vec_model = Word2Vec.load(model_path)\n",
    "# Check if the model file exists before trying to load\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Error: Model file not found at '{model_path}'\")\n",
    "    print(\"Please ensure the Word2Vec training cell ran successfully and saved the model.\")\n",
    "else:\n",
    "    print(f\"Loading Word2Vec model from '{model_path}'...\")\n",
    "    # Load the trained model\n",
    "    model = Word2Vec.load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # 1. Inspect the Vocabulary\n",
    "    print(\"\\n--- Vocabulary Inspection ---\")\n",
    "    vocab_list = model.wv.index_to_key # List of words in the vocabulary\n",
    "    vocab_size = len(vocab_list)\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    if vocab_size > 0:\n",
    "        print(f\"First 20 words in vocabulary: {vocab_list[:20]}\")\n",
    "    else:\n",
    "        print(\"Vocabulary is empty.\")\n",
    "\n",
    "    # Check if the model has a vocabulary before proceeding\n",
    "    if vocab_size > 0:\n",
    "        # 2. Get the Vector for a Specific Word\n",
    "        print(\"\\n--- Word Vector Inspection ---\")\n",
    "        # Pick a word likely to be in the vocab (use the first word if possible)\n",
    "        sample_word = vocab_list[0]\n",
    "        try:\n",
    "            vector = model.wv[sample_word]\n",
    "            print(f\"Vector for the word '{sample_word}':\")\n",
    "            print(f\"  Dimensions: {model.vector_size}\") # Or vector.shape[0]\n",
    "            # Display only the first few dimensions for brevity\n",
    "            print(f\"  First 10 dimensions: {np.round(vector[:10], 4)}\")\n",
    "        except KeyError:\n",
    "            # This should ideally not happen if we pick from vocab_list, but good practice\n",
    "            print(f\"Could not retrieve vector for '{sample_word}' (unexpected error).\")\n",
    "\n",
    "        # 3. Find Similar Words (Most Informative)\n",
    "        print(\"\\n--- Word Similarity Inspection ---\")\n",
    "        for word in words_to_inspect:\n",
    "            if word in model.wv: # Check if the word exists in the model's vocabulary\n",
    "                try:\n",
    "                    # Get the most similar words (returns list of tuples: (word, similarity_score))\n",
    "                    similar_words = model.wv.most_similar(word, topn=num_similar)\n",
    "                    print(f\"Words most similar to '{word}':\")\n",
    "                    for similar_word, score in similar_words:\n",
    "                        print(f\"  - {similar_word} (Score: {score:.4f})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not find similar words for '{word}': {e}\")\n",
    "            else:\n",
    "                print(f\"Word '{word}' is not in the model's vocabulary.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping vector and similarity inspection because the vocabulary is empty.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ffbe2-aef6-4a68-b583-ee73b08e7051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "403ce02f-890c-4c9b-a61d-297bef8bc08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer initialized with filters: '\t\n",
      "'\n",
      "MAX_NUM_WORDS: 100000, OOV_TOKEN: '<OOV>'\n"
     ]
    }
   ],
   "source": [
    "# --- MonographInitial.ipynb - MODIFIED Cell 9 ---\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "\n",
    "# Define a simpler filter. By default, Tokenizer splits on spaces and punctuation in the filters string.\n",
    "# Removing most punctuation from filters means it will only split on tabs and newlines,\n",
    "# preserving tokens that contain characters like <, >, or other symbols.\n",
    "TOKENIZER_FILTERS = '\\t\\n' # Minimal filter: only remove tabs and newlines\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=OOV_TOKEN, filters=TOKENIZER_FILTERS)\n",
    "\n",
    "print(f\"Tokenizer initialized with filters: '{TOKENIZER_FILTERS}'\")\n",
    "print(f\"MAX_NUM_WORDS: {MAX_NUM_WORDS}, OOV_TOKEN: '{OOV_TOKEN}'\")\n",
    "\n",
    "# --- End of MODIFIED Cell 9 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e690bd8e-f3a8-4ac4-8bb9-008ebd703451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Cell 10: Tokenizer fitting and saving...\n",
      "Recreated all_texts for tokenizer fitting. Size: 100000\n",
      "First text in all_texts for tokenizer: <start> Worcester breakfast club for veterans gives hunger its marching orders <end>\n",
      "Second text in all_texts for tokenizer: <start> Jumpshot Gives Marketers Renewed Visibility Into Paid and Organic Keywords With Launch of Jumpshot Elite <end>\n",
      "Tokenizer initialized with custom filters: '\t\n",
      "'\n",
      "Fitting tokenizer on all_texts...\n",
      "Tokenizer fitting complete.\n",
      "\n",
      "--- Verification of Tokenizer after Fit ---\n",
      "Tokenizer fitted vocabulary size: 786177\n",
      "Does tokenizer vocabulary contain '<start>'? True\n",
      "Does tokenizer vocabulary contain '<end>'? True\n",
      "ID for '<start>': 36\n",
      "ID for '<end>': 37\n",
      "\n",
      "Verification successful. Proceeding to save tokenizer.\n",
      "Tokenizer successfully saved to 'data/tokenizer.pkl'\n",
      "--- End Verification/Save Process ---\n"
     ]
    }
   ],
   "source": [
    "# --- MonographInitial.ipynb - CORRECTED Cell 10 ---\n",
    "\n",
    "# Ensure necessary variables are defined from previous cells or define them here for clarity\n",
    "# title_list, content_list must come from Cell 3 (which got them from the pickle saved by Cell 2)\n",
    "# MAX_NUM_WORDS, OOV_TOKEN must come from Cell 9\n",
    "# TOKENIZER_FILTERS must come from Cell 9\n",
    "# START_TOKEN, END_TOKEN must come from Cell 13 or define them here\n",
    "\n",
    "# Define tokens and key parameters again for robustness in this cell\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "# Assuming MAX_NUM_WORDS, OOV_TOKEN, TOKENIZER_FILTERS are defined in Cell 9\n",
    "\n",
    "\n",
    "print(\"Executing Cell 10: Tokenizer fitting and saving...\")\n",
    "\n",
    "# --- Explicitly recreate all_texts RIGHT NOW ---\n",
    "# This ensures we are using the most recent title_list and content_list\n",
    "if 'title_list' in globals() and 'content_list' in globals():\n",
    "    all_texts_for_tokenizer = title_list + content_list\n",
    "    print(f\"Recreated all_texts for tokenizer fitting. Size: {len(all_texts_for_tokenizer)}\")\n",
    "    # Optional: Verify first few items contain tokens\n",
    "    if len(all_texts_for_tokenizer) > 0:\n",
    "        print(f\"First text in all_texts for tokenizer: {all_texts_for_tokenizer[0]}\")\n",
    "        print(f\"Second text in all_texts for tokenizer: {all_texts_for_tokenizer[1]}\")\n",
    "else:\n",
    "    print(\"Error: title_list or content_list not found. Cannot create all_texts for tokenizer.\")\n",
    "    # You might want to stop execution here if the lists aren't found\n",
    "    # exit() # You can uncomment this to stop the notebook execution if data is missing\n",
    "\n",
    "\n",
    "# --- Initialize tokenizer with the custom filters ---\n",
    "# Ensure MAX_NUM_WORDS, OOV_TOKEN, and TOKENIZER_FILTERS are defined (should be from Cell 9)\n",
    "if 'MAX_NUM_WORDS' not in globals() or 'OOV_TOKEN' not in globals() or 'TOKENIZER_FILTERS' not in globals():\n",
    "     print(\"Error: MAX_NUM_WORDS, OOV_TOKEN, or TOKENIZER_FILTERS not defined (from Cell 9). Cannot initialize tokenizer.\")\n",
    "     # exit() # You can uncomment this to stop the notebook execution if parameters are missing\n",
    "\n",
    "# This is the corrected line: Include the filters argument\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=OOV_TOKEN, filters=TOKENIZER_FILTERS)\n",
    "print(f\"Tokenizer initialized with custom filters: '{TOKENIZER_FILTERS}'\") # Added print detail\n",
    "\n",
    "# --- Fit the tokenizer using the explicitly recreated all_texts ---\n",
    "print(\"Fitting tokenizer on all_texts...\")\n",
    "# Check if all_texts_for_tokenizer was successfully created\n",
    "if 'all_texts_for_tokenizer' in globals() and all_texts_for_tokenizer:\n",
    "    tokenizer.fit_on_texts(all_texts_for_tokenizer)\n",
    "    print(\"Tokenizer fitting complete.\")\n",
    "else:\n",
    "    print(\"Error: all_texts_for_tokenizer is not available or empty. Cannot fit tokenizer.\")\n",
    "    # exit() # You can uncomment this to stop if fitting failed\n",
    "\n",
    "\n",
    "# --- Verification of Tokenizer Immediately After Fit ---\n",
    "# These checks need START_TOKEN and END_TOKEN defined (from definitions above or Cell 13)\n",
    "if 'START_TOKEN' in globals() and 'END_TOKEN' in globals() and 'tokenizer' in globals():\n",
    "     print(\"\\n--- Verification of Tokenizer after Fit ---\")\n",
    "     print(f\"Tokenizer fitted vocabulary size: {len(tokenizer.word_index)}\")\n",
    "     # Note: Check in tokenizer.word_index, which contains all words found before filtering by num_words\n",
    "     contains_start = START_TOKEN in tokenizer.word_index\n",
    "     contains_end = END_TOKEN in tokenizer.word_index\n",
    "     print(f\"Does tokenizer vocabulary contain '{START_TOKEN}'? {contains_start}\")\n",
    "     print(f\"Does tokenizer vocabulary contain '{END_TOKEN}'? {contains_end}\")\n",
    "\n",
    "     start_id_check = tokenizer.word_index.get(START_TOKEN, None) # Use None default here\n",
    "     end_id_check = tokenizer.word_index.get(END_TOKEN, None) # Use None default here\n",
    "     print(f\"ID for '{START_TOKEN}': {start_id_check}\")\n",
    "     print(f\"ID for '{END_TOKEN}': {end_id_check}\")\n",
    "\n",
    "     # --- Save the tokenizer ONLY IF verification passes ---\n",
    "     # It's also good to check if the IDs are within the VOCAB_SIZE range IF you need them there\n",
    "     # But for this specific check, just verifying they exist and have IDs is enough.\n",
    "     if contains_start and contains_end and start_id_check is not None and end_id_check is not None:\n",
    "          print(\"\\nVerification successful. Proceeding to save tokenizer.\")\n",
    "          tokenizer_path = 'data/tokenizer.pkl' # Define the path\n",
    "          os.makedirs('data', exist_ok=True) # Ensure the 'data' directory exists\n",
    "          with open(tokenizer_path, 'wb') as handle:\n",
    "              pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "          print(f\"Tokenizer successfully saved to '{tokenizer_path}'\")\n",
    "     else:\n",
    "          print(\"\\nVerification failed. Tokenizer does NOT contain START/END tokens (or IDs are None). Not saving tokenizer.\")\n",
    "          print(\"This usually means the data loading (Cell 2) didn't include the tokens correctly, or the filters (Cell 9) were not applied/set up correctly.\") # More detailed error message\n",
    "\n",
    "     print(\"--- End Verification/Save Process ---\")\n",
    "\n",
    "else:\n",
    "     # This else is for the initial check if START_TOKEN/END_TOKEN/tokenizer are defined before verification block\n",
    "     print(\"\\nWarning: Tokenizer or necessary tokens (START_TOKEN, END_TOKEN) not defined before verification. Skipping tokenizer verification and saving.\")\n",
    "     print(\"Please ensure Cell 9 and preceding cells defining tokens are run.\")\n",
    "\n",
    "\n",
    "# --- End of CORRECTED Cell 10 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b251f-e04e-401d-9400-9971aa08a434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "075231a1-b47f-42aa-a42b-c2e432c53a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = MAX_NUM_WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ff41f5-3ff6-432c-a753-5a8c355bdb91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer created. Vocabulary size: 100000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer created. Vocabulary size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34abc4-b06b-441d-add2-58bec2863243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4461450-8fce-4b80-a1e6-279c6af15fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8499094-75c0-4306-a635-088563f4c677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb247012-1256-437f-9b40-e9dda992df07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification of Tokenizer after Definitions ---\n",
      "Tokenizer fitted vocabulary size: 786177\n",
      "Does tokenizer vocabulary contain '<start>'? True\n",
      "Does tokenizer vocabulary contain '<end>'? True\n",
      "ID for '<start>': 36\n",
      "ID for '<end>': 37\n",
      "--- End Verification ---\n"
     ]
    }
   ],
   "source": [
    "# --- Add this as a NEW cell AFTER Cell 13 in MonographInitial.ipynb ---\n",
    "\n",
    "print(\"\\n--- Verification of Tokenizer after Definitions ---\")\n",
    "# Check if tokenizer and necessary tokens are defined BEFORE using them in checks\n",
    "if 'tokenizer' in globals() and 'START_TOKEN' in globals() and 'END_TOKEN' in globals():\n",
    "     print(f\"Tokenizer fitted vocabulary size: {len(tokenizer.word_index)}\")\n",
    "     print(f\"Does tokenizer vocabulary contain '{START_TOKEN}'? {START_TOKEN in tokenizer.word_index}\")\n",
    "     print(f\"Does tokenizer vocabulary contain '{END_TOKEN}'? {END_TOKEN in tokenizer.word_index}\")\n",
    "     start_id_check = tokenizer.word_index.get(START_TOKEN)\n",
    "     end_id_check = tokenizer.word_index.get(END_TOKEN)\n",
    "     print(f\"ID for '{START_TOKEN}': {start_id_check}\")\n",
    "     print(f\"ID for '{END_TOKEN}': {end_id_check}\")\n",
    "else:\n",
    "     print(\"Warning: Tokenizer or necessary tokens not defined. Cannot perform verification.\")\n",
    "\n",
    "print(\"--- End Verification ---\")\n",
    "\n",
    "# --- End of new cell ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25ab8517-ae72-43d2-883d-06733c382a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = word2vec_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda6ddec-bcc1-47e3-b9b3-df66f2b2e789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Fill the matrix with Word2Vec vectors\n",
    "# Start from index 1 because index 0 is for padding\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < VOCAB_SIZE: # Ensure we stay within our desired vocabulary size\n",
    "        # Check if the word exists in the Word2Vec model's vocabulary\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]\n",
    "        else:\n",
    "            # Optional: Handle words not found in Word2Vec (e.g., leave as zeros, or random init)\n",
    "            # print(f\"Warning: Word '{word}' not found in Word2Vec vocabulary. Initializing with zeros.\")\n",
    "            pass # Keep as zero if initialized with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96055399-2dcb-4c1a-ad30-8ffd6d9daedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f0d33a9-dbc3-4d86-b554-f00731e8a31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ARTICLE_LEN =500 # Example length\n",
    "MAX_HEADLINE_LEN = 50 # Example length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ed9d1d1-aab5-43e0-ae7e-5b2c20361b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Size set to: 256\n",
      "Dropout Rate set to: 0.2\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(MAX_ARTICLE_LEN,), name='encoder_input')\n",
    "\n",
    "\n",
    "RNN_SIZE = 256\n",
    "\n",
    "\n",
    "DROPOUT_RATE = 0.2 \n",
    "\n",
    "print(f\"RNN Size set to: {RNN_SIZE}\")\n",
    "print(f\"Dropout Rate set to: {DROPOUT_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a23b349-0da1-4c23-92bb-d258c0caf72d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Size set to: 256\n",
      "Dropout Rate set to: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 15:35:34.184594: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-05-21 15:35:34.184618: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-05-21 15:35:34.184633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (daniel-diaz-Latitude-3520): /proc/driver/nvidia/version does not exist\n",
      "2025-05-21 15:35:34.186645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq model structure defined.\n",
      "Model: \"seq2seq_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " shared_embedding0 (Embedding)  (None, 500, 100)     10000000    ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder_lstm_1 (LSTM)          [(None, 500, 256),   365568      ['shared_embedding0[0][0]']      \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm_2 (LSTM)          [(None, 500, 256),   525312      ['encoder_lstm_1[0][0]']         \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " shared_embedding1 (Embedding)  (None, 50, 100)      10000000    ['decoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " encoder_lstm_3 (LSTM)          [(None, 500, 256),   525312      ['encoder_lstm_2[0][0]']         \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm_1 (LSTM)          [(None, 50, 256),    365568      ['shared_embedding1[0][0]',      \n",
      "                                 (None, 256),                     'encoder_lstm_3[0][1]',         \n",
      "                                 (None, 256)]                     'encoder_lstm_3[0][2]']         \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 50, 100000)   25700000    ['decoder_lstm_1[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,481,760\n",
      "Trainable params: 47,481,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- In Cell 23 ---\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Keep your MAX_ARTICLE_LEN and MAX_HEADLINE_LEN definitions as they are\n",
    "MAX_ARTICLE_LEN =500 # Example length\n",
    "MAX_HEADLINE_LEN = 50 # Example length\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_ARTICLE_LEN,), name='encoder_input')\n",
    "\n",
    "# Keep your RNN_SIZE and DROPOUT_RATE definitions as they are\n",
    "RNN_SIZE = 256\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "print(f\"RNN Size set to: {RNN_SIZE}\")\n",
    "print(f\"Dropout Rate set to: {DROPOUT_RATE}\")\n",
    "\n",
    "# Keep this as it is (encoder embedding)\n",
    "encoder_embedding = Embedding( input_dim=VOCAB_SIZE,\n",
    "                              output_dim=EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              trainable=True,\n",
    "                              mask_zero=True,\n",
    "                              name='shared_embedding0')(encoder_inputs)\n",
    "\n",
    "# Keep encoder LSTMs as they are (they already have return_state=True)\n",
    "encoder_lstm1 = LSTM(RNN_SIZE, return_sequences=True, return_state=True, name='encoder_lstm_1')(encoder_embedding)\n",
    "encoder_lstm2 = LSTM(RNN_SIZE, return_sequences=True, return_state=True, name='encoder_lstm_2')(encoder_lstm1[0])\n",
    "encoder_lstm3 = LSTM(RNN_SIZE, return_sequences=True, return_state=True, name='encoder_lstm_3')(encoder_lstm2[0])\n",
    "\n",
    "# Keep encoder states as they are (take states from the *last* LSTM)\n",
    "encoder_states = [encoder_lstm3[1], encoder_lstm3[2]]\n",
    "\n",
    "# Keep decoder_inputs as they are\n",
    "decoder_inputs = Input(shape=(MAX_HEADLINE_LEN,), name='decoder_input')\n",
    "\n",
    "# Keep decoder embedding as it is (ensure name matches inference lookup: 'shared_embedding1')\n",
    "decoder_embedding = Embedding(input_dim=VOCAB_SIZE,\n",
    "                              output_dim=EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              trainable=True,\n",
    "                              mask_zero=True,\n",
    "                              name='shared_embedding1')(decoder_inputs)\n",
    "\n",
    "# THIS WAS THE LINE TO CHANGE - YOU NEED TO SELECT THE SEQUENCE OUTPUT [0]\n",
    "decoder_lstm1 = LSTM(RNN_SIZE,\n",
    "                     return_sequences=True,\n",
    "                     return_state=True,\n",
    "                     name='decoder_lstm_1')(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Corrected line: Pass only the sequence output [0] to the Dense layer\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax', name='output_layer')(decoder_lstm1[0]) # <--- FIXED THIS LINE\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_dense, name='seq2seq_model')\n",
    "\n",
    "print(\"Seq2Seq model structure defined.\")\n",
    "model.summary() # Look at the summary again to confirm shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401bd76-6301-4e7a-8ec1-f09f411a0a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee54af7f-548b-4f35-9ae8-2780dda256b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_input_sequences = tokenizer.texts_to_sequences(content_list)\n",
    "decoder_input_sequences = tokenizer.texts_to_sequences(title_list)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(title_list)\n",
    "encoder_input_sequences = pad_sequences(encoder_input_sequences, maxlen=MAX_ARTICLE_LEN, padding='post')\n",
    "decoder_input_sequences = pad_sequences(decoder_input_sequences, maxlen=MAX_HEADLINE_LEN, padding='post')\n",
    "decoder_target_sequences = pad_sequences(decoder_target_sequences, maxlen=MAX_HEADLINE_LEN, padding='post')\n",
    "START_TOKEN_ID = tokenizer.word_index.get(START_TOKEN, 1)\n",
    "END_TOKEN_ID = tokenizer.word_index.get(END_TOKEN, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4bccb5b-b42a-402d-895a-48cc52f393cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Checkpoint file not found at 'training_checkpoints/epoch_50_val_loss_1.4797.h5'\n",
      "Cannot resume training. Please check the path or train from scratch.\n"
     ]
    }
   ],
   "source": [
    "# --- NEW Cell: Load Model from Checkpoint to Resume Training ---\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define the path to the checkpoint file you want to resume from\n",
    "# Make sure this path is correct!\n",
    "resume_checkpoint_path = 'training_checkpoints/epoch_50_val_loss_1.4797.h5' # <--- Verify this path\n",
    "\n",
    "if not os.path.exists(resume_checkpoint_path):\n",
    "    print(f\"Error: Checkpoint file not found at '{resume_checkpoint_path}'\")\n",
    "    print(\"Cannot resume training. Please check the path or train from scratch.\")\n",
    "    # You might want to stop execution here if the checkpoint is essential\n",
    "else:\n",
    "    print(f\"Loading model from checkpoint: '{resume_checkpoint_path}' to resume training...\")\n",
    "\n",
    "    # Load the entire model, including architecture, weights, and optimizer state\n",
    "    # Keras automatically preserves the compilation state when loading the full model\n",
    "    model = tf.keras.models.load_model(resume_checkpoint_path)\n",
    "\n",
    "    print(\"Model loaded successfully. Ready to resume training.\")\n",
    "\n",
    "    # Optional: Print current optimizer state if you want to inspect\n",
    "    # print(f\"Optimizer state loaded: {model.optimizer.get_config()}\")\n",
    "\n",
    "# --- End of NEW Cell ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8257f-45df-4c99-9e15-4fbabef052bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint callback setup complete.\n",
      "Checkpoints will be saved to: ./training_checkpoints\n",
      "Filename pattern: epoch_{epoch:02d}_val_loss_{val_loss:.4f}.h5\n",
      "Model compiled (or confirmed compilation state).\n",
      "Resuming model training from epoch 51 up to epoch 100...\n",
      "Epoch 51/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7e73bf4dbb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7e73bf4dbb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "400/400 [==============================] - ETA: 0s - loss: 7.2058  WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7e7353bd9dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7e7353bd9dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "Epoch 51: saving model to ./training_checkpoints/epoch_50_val_loss_6.5045.h5\n",
      "400/400 [==============================] - 26348s 66s/step - loss: 7.2058 - val_loss: 6.5045\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 6.0094  \n",
      "Epoch 52: saving model to ./training_checkpoints/epoch_52_val_loss_5.6437.h5\n",
      "400/400 [==============================] - 33040s 83s/step - loss: 6.0094 - val_loss: 5.6437\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 5.3119 \n",
      "Epoch 53: saving model to ./training_checkpoints/epoch_53_val_loss_5.0838.h5\n",
      "400/400 [==============================] - 4762s 12s/step - loss: 5.3119 - val_loss: 5.0838\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - ETA: 0s - loss: 4.7940 \n",
      "Epoch 54: saving model to ./training_checkpoints/epoch_54_val_loss_4.6325.h5\n",
      "400/400 [==============================] - 17466s 44s/step - loss: 4.7940 - val_loss: 4.6325\n",
      "Epoch 55/100\n",
      " 21/400 [>.............................] - ETA: 1:14:24 - loss: 4.5754"
     ]
    }
   ],
   "source": [
    "# --- MonographInitial.ipynb - Cell 24 (COMPLETE code for Resuming Training with Checkpointing and Plotting) ---\n",
    "\n",
    "# Ensure you have matplotlib imported at the top of your notebook (e.g., in Cell 1)\n",
    "import matplotlib.pyplot as plt # Added import for plotting here for clarity, but should be at the top\n",
    "import os # Needed for directory creation and path joining\n",
    "\n",
    "# Make sure encoder_input_sequences, decoder_input_sequences,\n",
    "# decoder_target_sequences are defined (should be from Cell 22).\n",
    "# Make sure MAX_ARTICLE_LEN, MAX_HEADLINE_LEN, BATCH_SIZE are defined (from earlier cells).\n",
    "# Make sure the 'model' variable is populated from Cell 23 (the loading cell).\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# --- Variable Definitions for Resuming/Training ---\n",
    "# If you want to train for an additional number of epochs:\n",
    "NEW_TOTAL_EPOCHS = 100 # Set your desired total number of epochs here (e.g., 50 + 50 = 100)\n",
    "\n",
    "# The epoch to start counting from (the one AFTER the loaded checkpoint's epoch number)\n",
    "# If you loaded epoch_50, you start from epoch 51 for the progress counter and callbacks\n",
    "STARTING_EPOCH_NUMBER = 50 # <--- Set this to the epoch number you just loaded FROM (50 in your case)\n",
    "\n",
    "\n",
    "# Ensure model is defined (loaded in Cell 23)\n",
    "if 'model' not in globals() or model is None:\n",
    "    print(\"Error: Model variable 'model' is not defined or is None. Please run Cell 23 to load the model.\")\n",
    "    # You might want to stop execution here if the model is essential\n",
    "    # exit() # Uncomment this line if you want to stop automatically\n",
    "\n",
    "\n",
    "# --- 1. Setup Model Checkpointing ---\n",
    "\n",
    "# Define the directory where checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define the filename format for checkpoints\n",
    "# Includes epoch number ({epoch:02d}) and validation loss ({val_loss:.4f})\n",
    "# The {epoch} placeholder will use the initial_epoch value correctly for numbering\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'epoch_{epoch:02d}_val_loss_{val_loss:.4f}.h5')\n",
    "\n",
    "# Create the ModelCheckpoint callback instance\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, # Set to True if you only want to save weights, False saves entire model\n",
    "    monitor='val_loss',      # Metric to monitor (included in filename)\n",
    "    mode='min',              # Mode for the monitored metric\n",
    "    save_best_only=False,    # <--- IMPORTANT: Set to False to save AFTER *every* epoch\n",
    "    save_freq='epoch',       # <--- IMPORTANT: Set to 'epoch' to trigger saving at the end of each epoch\n",
    "    verbose=1                # Print messages when saving a checkpoint\n",
    ")\n",
    "\n",
    "print(\"ModelCheckpoint callback setup complete.\")\n",
    "print(f\"Checkpoints will be saved to: {checkpoint_dir}\")\n",
    "print(f\"Filename pattern: {os.path.basename(checkpoint_filepath)}\")\n",
    "\n",
    "\n",
    "# --- 2. Compile the model (if necessary - loading full model often preserves compile state) ---\n",
    "# You generally don't *need* to re-compile if loading the full model using tf.keras.models.load_model,\n",
    "# as the compilation state (optimizer, loss, metrics) is saved.\n",
    "# However, calling compile again doesn't hurt if the arguments are the same, and is necessary\n",
    "# if you changed compile args or if you loaded weights only.\n",
    "# Let's keep it here but note it might be redundant after loading the full model.\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "print(\"Model compiled (or confirmed compilation state).\")\n",
    "\n",
    "\n",
    "# --- 3. Start training with the callback ---\n",
    "print(f\"Resuming model training from epoch {STARTING_EPOCH_NUMBER + 1} up to epoch {NEW_TOTAL_EPOCHS}...\")\n",
    "\n",
    "history = model.fit([encoder_input_sequences, decoder_input_sequences],\n",
    "                    decoder_target_sequences,\n",
    "                    epochs=NEW_TOTAL_EPOCHS,        # <--- Train up to this total epoch number\n",
    "                    initial_epoch=STARTING_EPOCH_NUMBER, # <--- Tell Keras to start counting epochs from here\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[model_checkpoint_callback]) # <--- Pass the callback list here\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# --- 4. Plot Loss History ---\n",
    "# The history object from fit will contain data from the start of the *resumed* training.\n",
    "# Plotting will show the loss from epoch STARTING_EPOCH_NUMBER+1 onwards.\n",
    "print(\"Plotting loss history for resumed training...\")\n",
    "\n",
    "# Get the loss values from the history object\n",
    "loss = history.history['loss']\n",
    "# Check if validation loss was recorded (it should be with validation_split=0.2)\n",
    "val_loss_available = 'val_loss' in history.history\n",
    "if val_loss_available:\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Adjust x-axis to start from the correct epoch number for the plot\n",
    "# The range should go from STARTING_EPOCH_NUMBER + 1 up to NEW_TOTAL_EPOCHS inclusive of epochs where data was recorded\n",
    "# The length of history.history['loss'] corresponds to the number of *new* epochs trained.\n",
    "# So if you trained 50 original + 50 new, the length is 50. The epoch numbers are STARTING_EPOCH_NUMBER + 1, ..., STARTING_EPOCH_NUMBER + len(loss)\n",
    "epochs_trained_in_this_run = len(loss)\n",
    "epochs_range_for_plot = range(STARTING_EPOCH_NUMBER + 1, STARTING_EPOCH_NUMBER + epochs_trained_in_this_run + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs_range_for_plot, loss, label='Training Loss')\n",
    "if val_loss_available:\n",
    "    plt.plot(epochs_range_for_plot, val_loss, label='Validation Loss')\n",
    "\n",
    "plt.title('Training and Validation Loss per Epoch (Resumed)') # Updated title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"Loss plot displayed.\")\n",
    "\n",
    "\n",
    "# --- 5. Optional: Save the final model explicitly ---\n",
    "# This is in addition to the epoch-specific checkpoints saved by the callback.\n",
    "# You can keep this commented out if you prefer relying on the checkpoint files.\n",
    "# If uncommented, this saves the state *after* the last epoch of the resumed training.\n",
    "# print(\"Saving final model...\")\n",
    "# final_model_save_path = 'seq2seq_headline_summarizer_model_final.h5'\n",
    "# # Ensure the data directory exists (it should from earlier cells, but good practice)\n",
    "# os.makedirs('data', exist_ok=True) # Assuming you want the final save in data dir\n",
    "# model.save(final_model_save_path) # Or specify a path like 'data/seq2seq_headline_summarizer_model_final.h5'\n",
    "# print(f\"Final model saved to '{final_model_save_path}'\")\n",
    "\n",
    "\n",
    "# --- End of MonographInitial.ipynb - Cell 24 COMPLETE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd2989-4292-4cdb-8904-ed08a4b2bcf2",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a24e5-e6c2-40bd-97de-8b34d230b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- START: Code for a NEW session ---\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- Define the same constants as training ---\n",
    "# Make sure these match the values used during training\n",
    "MAX_RECORDS_TO_LOAD = 100000 # Used for data loading initially, needed for context\n",
    "MAX_ARTICLE_LEN = 500       # Must match training\n",
    "MAX_HEADLINE_LEN = 50       # Must match training\n",
    "MAX_NUM_WORDS = 9999      # Must match tokenizer setup\n",
    "OOV_TOKEN = \"<OOV>\"         # Must match tokenizer setup\n",
    "START_TOKEN = \"<start>\"     # Must match tokenizer setup and data prep\n",
    "END_TOKEN = \"<end>\"         # Must match tokenizer setup and data prep\n",
    "RNN_SIZE = 256              # Must match training\n",
    "# Note: EMBEDDING_DIM comes from the loaded model's embedding layer\n",
    "\n",
    "# --- Load or Recreate the Tokenizer ---\n",
    "# Ideally, save the tokenizer after fitting it in the original notebook\n",
    "tokenizer_path = 'data/tokenizer.pkl' # Recommended: Save your tokenizer!\n",
    "\n",
    "# --- Example: Save the tokenizer after fitting (run this ONCE in the original notebook) ---\n",
    "# In the original notebook, after running cell 10:\n",
    "# with open(tokenizer_path, 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# --- Load the tokenizer in the new session ---\n",
    "if os.path.exists(tokenizer_path):\n",
    "    print(f\"Loading tokenizer from '{tokenizer_path}'...\")\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    print(\"Tokenizer loaded.\")\n",
    "else:\n",
    "    # If tokenizer was not saved, you need to recreate it by fitting on the same data\n",
    "    # This requires loading the data again or having the 'all_texts' list available.\n",
    "    # Loading the saved tokenizer is much more reliable.\n",
    "    print(f\"Error: Tokenizer file not found at '{tokenizer_path}'.\")\n",
    "    print(\"Please save the tokenizer after fitting it in the original notebook.\")\n",
    "    # Fallback (less reliable if data/preprocessing changes):\n",
    "    # You'd need to load 'title_content_pair.pkl' and rebuild all_texts here\n",
    "    # all_texts = ... # Load data and combine titles/content\n",
    "    # tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=OOV_TOKEN)\n",
    "    # tokenizer.fit_on_texts(all_texts)\n",
    "    # print(\"Tokenizer recreated from data (less reliable than loading).\")\n",
    "    # VOCAB_SIZE = min(tokenizer.num_words, len(tokenizer.word_index) + 1) # +1 for OOV/padding if not included in num_words\n",
    "    exit() # Stop if tokenizer loading fails\n",
    "\n",
    "# Ensure VOCAB_SIZE is consistent with the tokenizer and training setup\n",
    "# If num_words was set, VOCAB_SIZE is usually num_words.\n",
    "# If num_words was None, it's len(tokenizer.word_index) + 1 (for OOV) or +2 (OOV and padding 0)\n",
    "# Given you set MAX_NUM_WORDS=9000 and OOV, VOCAB_SIZE should be 9000.\n",
    "VOCAB_SIZE = MAX_NUM_WORDS # Or min(MAX_NUM_WORDS, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Get token IDs (ensure they match the tokenizer's mapping)\n",
    "START_TOKEN_ID = tokenizer.word_index.get(START_TOKEN, 1) # get returns default if not found\n",
    "END_TOKEN_ID = tokenizer.word_index.get(END_TOKEN, 2)   # These might be 1, 2 depending on tokenizer fit order\n",
    "\n",
    "print(f\"Tokenizer vocabulary size (MAX_NUM_WORDS): {VOCAB_SIZE}\")\n",
    "print(f\"Start Token ID: {START_TOKEN_ID}\")\n",
    "print(f\"End Token ID: {END_TOKEN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6716258-1163-4df4-a306-4075303ea62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- In a new cell, after training ---\n",
    "\n",
    "# Assuming you have the trained `model`, `tokenizer`, `MAX_ARTICLE_LEN`, `MAX_HEADLINE_LEN`,\n",
    "# START_TOKEN_ID, END_TOKEN_ID, VOCAB_SIZE (from the previous steps)\n",
    "# Note: Make sure START_TOKEN and END_TOKEN are added to your titles *before* tokenization\n",
    "# and that VOCAB_SIZE is correctly set to MAX_NUM_WORDS as discussed in the previous answer.\n",
    "\n",
    "print(\"Setting up inference models (reusing trained layers)...\")\n",
    "\n",
    "# Get the layers from the trained model\n",
    "# Use the exact names from the traceback: 'shared_embedding0', 'shared_embedding1', etc.\n",
    "encoder_embedding_layer = model.get_layer('shared_embedding0')\n",
    "encoder_lstm1_layer = model.get_layer('encoder_lstm_1')\n",
    "encoder_lstm2_layer = model.get_layer('encoder_lstm_2')\n",
    "encoder_lstm3_layer = model.get_layer('encoder_lstm_3')\n",
    "\n",
    "decoder_embedding_layer = model.get_layer('shared_embedding1')\n",
    "decoder_lstm_layer = model.get_layer('decoder_lstm_1') # This is the trained decoder LSTM layer\n",
    "decoder_dense_layer = model.get_layer('output_layer')\n",
    "\n",
    "# --- Encoder Inference Model ---\n",
    "# Input: Define a new Input tensor for the encoder inference model\n",
    "encoder_inputs_inf = Input(shape=(MAX_ARTICLE_LEN,), name='encoder_input_inf')\n",
    "\n",
    "# Chain the layers using the *trained* layer instances\n",
    "encoder_embedding_out = encoder_embedding_layer(encoder_inputs_inf)\n",
    "\n",
    "# Pass through the encoder LSTMs to get the final states\n",
    "# Since the original model had multiple stacked LSTMs and output the last one's states,\n",
    "# we need to propagate the outputs through the layers.\n",
    "# The outputs of LSTMs (when return_sequences=True) are the sequences, used as input to the next LSTM.\n",
    "# The return_state=True on the original LSTMs allows us to access their states.\n",
    "encoder_lstm1_output, state_h1_enc_inf, state_c1_enc_inf = encoder_lstm1_layer(encoder_embedding_out)\n",
    "encoder_lstm2_output, state_h2_enc_inf, state_c2_enc_inf = encoder_lstm2_layer(encoder_lstm1_output)\n",
    "encoder_lstm3_output, state_h3_enc_inf, state_c3_enc_inf = encoder_lstm3_layer(encoder_lstm2_output)\n",
    "\n",
    "# The encoder inference model outputs the states of the *last* encoder LSTM\n",
    "encoder_states_inf = [state_h3_enc_inf, state_c3_enc_inf]\n",
    "encoder_model = Model(encoder_inputs_inf, encoder_states_inf)\n",
    "\n",
    "print(\"Encoder Inference Model set up.\")\n",
    "\n",
    "\n",
    "# --- Decoder Inference Model ---\n",
    "# This model takes the previously predicted word and the decoder states,\n",
    "# and outputs the next word probabilities and the new decoder states.\n",
    "\n",
    "# Inputs for the decoder inference model\n",
    "# 1. Input for the single word predicted at the previous step (or START token initially)\n",
    "decoder_single_word_input_inf = Input(shape=(1,), name='decoder_single_word_input_inf')\n",
    "\n",
    "# 2. Inputs for the previous decoder states (output from the encoder or previous decoder step)\n",
    "decoder_state_input_h_inf = Input(shape=(RNN_SIZE,), name='decoder_state_input_h_inf')\n",
    "decoder_state_input_c_inf = Input(shape=(RNN_SIZE,), name='decoder_state_input_c_inf')\n",
    "decoder_states_inputs_inf = [decoder_state_input_h_inf, decoder_state_input_c_inf] # List of state inputs\n",
    "\n",
    "# Process the single word input: Embedding layer\n",
    "decoder_single_word_embedding_inf = decoder_embedding_layer(decoder_single_word_input_inf) # Use the trained embedding layer\n",
    "\n",
    "# Pass through the *trained* decoder LSTM layer\n",
    "# Provide the previous states as `initial_state`.\n",
    "# Since the input shape is (None, 1, EMBEDDING_DIM), and `return_sequences=True` on the layer instance,\n",
    "# the output sequence will have shape (None, 1, RNN_SIZE).\n",
    "# The layer also returns the new states because `return_state=True` was on the original layer.\n",
    "decoder_outputs_inf, state_h_dec_inf, state_c_dec_inf = decoder_lstm_layer(\n",
    "    decoder_single_word_embedding_inf, initial_state=decoder_states_inputs_inf\n",
    ")\n",
    "\n",
    "# Apply the *trained* dense output layer\n",
    "# This layer was applied to the output sequence in the training model.\n",
    "# When applied to the (None, 1, RNN_SIZE) output from the inference LSTM step,\n",
    "# it will produce an output shape of (None, 1, VOCAB_SIZE).\n",
    "decoder_pred_output_sequence = decoder_dense_layer(decoder_outputs_inf)\n",
    "\n",
    "# For step-by-step inference, we need the prediction for the *current* timestep.\n",
    "# Since the sequence length is 1, we take the first (and only) element of the output sequence.\n",
    "decoder_pred_output_inf = decoder_pred_output_sequence[:, 0, :] # Slice to get shape (None, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# The decoder inference model outputs the word probabilities for the current step\n",
    "# and the new decoder states for the next step.\n",
    "decoder_model = Model(\n",
    "    [decoder_single_word_input_inf] + decoder_states_inputs_inf,\n",
    "    [decoder_pred_output_inf] + [state_h_dec_inf, state_c_dec_inf] # Output the predicted probs and new states\n",
    ")\n",
    "\n",
    "print(\"Decoder Inference Model set up.\")\n",
    "print(\"Inference models built by reusing trained layers. No manual weight transfer needed.\")\n",
    "\n",
    "# Note: You don't need explicit build() calls or set_weights() here\n",
    "# because you are using layers that were already built and have weights\n",
    "# from the original `model` during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068b70a-f9be-4f8f-aa87-b4cd119fdf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e2f3f-eb9d-40f7-8fc9-c3c69281fc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646560f-e3df-47cf-b37d-f6cc5d5205f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
