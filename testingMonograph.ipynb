{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5cd117-26e5-4028-ba03-a0c8448435b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 11:29:01.988547: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-23 11:29:02.420520: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-23 11:29:02.499591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-05-23 11:29:02.499610: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-05-23 11:29:03.806221: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-23 11:29:03.806855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-23 11:29:03.806869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic imports complete.\n",
      "Constants defined.\n",
      "NLTK Punkt tokenizer found.\n",
      "NLTK setup complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import Counter # May not be needed for inference, but keeping for consistency\n",
    "import keras # Often imported with TF, but good to be explicit\n",
    "import Levenshtein # Used for Levenshtein Distance (evaluation)\n",
    "import pickle\n",
    "import json\n",
    "# zipfile was not used in the provided code, can be removed\n",
    "from tqdm.auto import tqdm # Useful for progress bars if processing large amounts of data later\n",
    "import os\n",
    "# gensim Word2Vec related imports might not be strictly needed if just loading the model and embeddings\n",
    "# but keeping them for completeness if you might use Word2Vec directly later.\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- Imports for Evaluation Metrics (if you plan to use them) ---\n",
    "# from rouge import Rouge # For ROUGE scores - install with pip if needed\n",
    "import nltk\n",
    "# import word_tokenize # Not directly used in decode/sequences_to_text, but useful generally\n",
    "# import corpus_bleu # For BLEU score\n",
    "# from scipy.spatial.distance import cosine # For Cosine Similarity\n",
    "\n",
    "\n",
    "print(\"Basic imports complete.\")\n",
    "\n",
    "# --- Define the same constants as training ---\n",
    "# Make sure these match the values used during training (refer to your original notebook)\n",
    "# MAX_RECORDS_TO_LOAD is only relevant for initial data loading, not inference setup\n",
    "MAX_ARTICLE_LEN = 500       # Must match training\n",
    "MAX_HEADLINE_LEN = 50       # Must match training\n",
    "MAX_NUM_WORDS = 100000      # <--- CHANGE THIS TO MATCH TRAINING NOTEBOOK\n",
    "OOV_TOKEN = \"<OOV>\"         # Must match tokenizer setup\n",
    "START_TOKEN = \"<start>\"     # Must match tokenizer setup and data prep (should be added to titles)\n",
    "END_TOKEN = \"<end>\"         # Must match tokenizer setup and data prep (should be added to titles)\n",
    "RNN_SIZE = 256              # Must match training\n",
    "\n",
    "print(\"Constants defined.\")\n",
    "# Note: EMBEDDING_DIM will be determined from the loaded model\n",
    "\n",
    "\n",
    "# --- Download necessary NLTK data if not already present (Corrected Error Handling) ---\n",
    "# This handles the LookupError raised when the resource is not found\n",
    "try:\n",
    "    # Try to find the punkt resource\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK Punkt tokenizer found.\")\n",
    "except LookupError: # Catch the actual error raised by nltk.data.find\n",
    "    print(\"NLTK Punkt tokenizer not found. Downloading...\")\n",
    "    # Download the punkt resource\n",
    "    nltk.download('punkt')\n",
    "    print(\"NLTK Punkt tokenizer downloaded.\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected errors during the check/download\n",
    "    print(f\"An unexpected error occurred during NLTK check/download: {e}\")\n",
    "\n",
    "\n",
    "print(\"NLTK setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eaa03d3-4bc6-49a5-b099-7e44cf50ca0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from 'data/tokenizer.pkl'...\n",
      "Tokenizer loaded successfully.\n",
      "Tokenizer vocabulary size: 100000\n",
      "Start Token ID: 36\n",
      "End Token ID: 37\n",
      "Reverse word index created.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = 'data/tokenizer.pkl' # Path where you saved your tokenizer\n",
    "\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    print(f\"Error: Tokenizer file not found at '{tokenizer_path}'.\")\n",
    "    print(\"Please ensure you saved the tokenizer in your original training notebook.\")\n",
    "    # You might want to exit or raise an error here if the tokenizer is essential\n",
    "else:\n",
    "    print(f\"Loading tokenizer from '{tokenizer_path}'...\")\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "    # Ensure VOCAB_SIZE is consistent with the tokenizer and training setup\n",
    "    # It should be MAX_NUM_WORDS if you set it.\n",
    "    VOCAB_SIZE = tokenizer.num_words # Use tokenizer's num_words attribute\n",
    "    if VOCAB_SIZE is None: # If num_words wasn't explicitly set, it's the full vocab + OOV\n",
    "         VOCAB_SIZE = len(tokenizer.word_index) + 1 # +1 for OOV token\n",
    "\n",
    "    # Get token IDs (ensure they match the tokenizer's mapping)\n",
    "    # Use .get() with a default in case tokens weren't in the top MAX_NUM_WORDS\n",
    "    START_TOKEN_ID = tokenizer.word_index.get(START_TOKEN, None)\n",
    "    END_TOKEN_ID = tokenizer.word_index.get(END_TOKEN, None)\n",
    "    # Handle cases where START/END might not be in the tokenizer vocab (shouldn't happen if added to data)\n",
    "    if START_TOKEN_ID is None or END_TOKEN_ID is None:\n",
    "         print(f\"Warning: START_TOKEN ('{START_TOKEN}') or END_TOKEN ('{END_TOKEN}') not found in tokenizer vocabulary.\")\n",
    "         # You might need to handle this case depending on your token IDs.\n",
    "         # Let's assume they exist if you added them to titles before fitting.\n",
    "         START_TOKEN_ID = tokenizer.word_index.get(START_TOKEN)\n",
    "         END_TOKEN_ID = tokenizer.word_index.get(END_TOKEN)\n",
    "\n",
    "\n",
    "    print(f\"Tokenizer vocabulary size: {VOCAB_SIZE}\")\n",
    "    print(f\"Start Token ID: {START_TOKEN_ID}\")\n",
    "    print(f\"End Token ID: {END_TOKEN_ID}\")\n",
    "\n",
    "    # Create reverse word index (ID to word) for converting output IDs back to text\n",
    "    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    print(\"Reverse word index created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c6f83b-7405-4706-8729-50fe62c8041e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from 'training_checkpoints/epoch_50_val_loss_1.4797.h5'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 11:29:06.772440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-05-23 11:29:06.772836: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-05-23 11:29:06.772851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (daniel-diaz-Latitude-3520): /proc/driver/nvidia/version does not exist\n",
      "2025-05-23 11:29:06.773775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Embedding Dimension from loaded model: 100\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3 (Load Trained Model) - MODIFIED PATH & SYNTAX FIX ---\n",
    "\n",
    "# Set the path to the specific checkpoint file you want to load\n",
    "# Use the exact path you found after running the training notebook\n",
    "model_save_path = 'training_checkpoints/epoch_50_val_loss_1.4797.h5' # <--- CHANGE THIS LINE\n",
    "\n",
    "if not os.path.exists(model_save_path):\n",
    "    print(f\"Error: Trained model file not found at '{model_save_path}'\")\n",
    "    print(\"Please ensure the path is correct and the training notebook saved this file.\")\n",
    "    # You might want to exit or raise an error here if the model is essential\n",
    "else:\n",
    "    print(f\"Loading trained model from '{model_save_path}'...\")\n",
    "    # Load the full model, including architecture and weights\n",
    "    loaded_model = tf.keras.models.load_model(model_save_path, compile=False) # compile=False is fine for inference\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Get the Embedding Dimension from the loaded model's embedding layer\n",
    "    try:\n",
    "        # Assuming the first embedding layer ('shared_embedding0') is the encoder one\n",
    "        # Or find it by layer type or name if you used specific names\n",
    "        embedding_layer = None\n",
    "        for layer in loaded_model.layers:\n",
    "            # You can also check by name if you're certain of the name:\n",
    "            # if layer.name == 'shared_embedding0':\n",
    "            #     embedding_layer = layer\n",
    "            #     break\n",
    "            if isinstance(layer, tf.keras.layers.Embedding):\n",
    "                 embedding_layer = layer\n",
    "                 break # Found the first embedding layer\n",
    "\n",
    "        if embedding_layer:\n",
    "            EMBEDDING_DIM = embedding_layer.output_dim\n",
    "            print(f\"Embedding Dimension from loaded model: {EMBEDDING_DIM}\")\n",
    "        else:\n",
    "            # --- SYNTAX ERROR FIXED HERE ---\n",
    "            print(f\"Error: Could not find an Embedding layer in the loaded model.\\n\") # <-- Corrected this line\n",
    "            # Fallback or error handling if embedding dim is critical\n",
    "            # This fallback might be needed if you change the model architecture significantly\n",
    "            # For your current model, finding the embedding layer should work.\n",
    "            EMBEDDING_DIM = 100 # Fallback to the value used during training if known\n",
    "            print(f\"Using fallback EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding dimension from loaded model: {e}\")\n",
    "        EMBEDDING_DIM = 100 # Fallback\n",
    "        print(f\"Using fallback EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "\n",
    "\n",
    "    # Optional: Display model summary to verify\n",
    "    # loaded_model.summary()\n",
    "\n",
    "# --- End of Cell 3 MODIFIED ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdfd180-ef08-4226-93d2-e6cdea5120fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up inference models from loaded model layers...\n",
      "Encoder Inference Model set up.\n",
      "Decoder Inference Model set up.\n",
      "Inference models built by reusing trained layers.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up inference models from loaded model layers...\")\n",
    "\n",
    "if 'loaded_model' not in locals():\n",
    "    print(\"Error: Trained model not loaded. Please run the previous cell.\")\n",
    "    # Exit or handle error\n",
    "\n",
    "\n",
    "try:\n",
    "    # Get the layers from the loaded model by the names you used in the original training code\n",
    "    encoder_embedding_layer = loaded_model.get_layer('shared_embedding0')\n",
    "    encoder_lstm1_layer = loaded_model.get_layer('encoder_lstm_1')\n",
    "    encoder_lstm2_layer = loaded_model.get_layer('encoder_lstm_2')\n",
    "    encoder_lstm3_layer = loaded_model.get_layer('encoder_lstm_3')\n",
    "\n",
    "    decoder_embedding_layer = loaded_model.get_layer('shared_embedding1')\n",
    "    decoder_lstm_layer = loaded_model.get_layer('decoder_lstm_1')\n",
    "    decoder_dense_layer = loaded_model.get_layer('output_layer')\n",
    "\n",
    "    # --- Encoder Inference Model ---\n",
    "    # Takes the input sequence and outputs the final encoder states\n",
    "    encoder_inputs_inf = tf.keras.Input(shape=(MAX_ARTICLE_LEN,), name='encoder_input_inf')\n",
    "    encoder_embedding_out = encoder_embedding_layer(encoder_inputs_inf)\n",
    "\n",
    "    # Propagate through encoder LSTMs to get the final states of the LAST LSTM\n",
    "    encoder_lstm1_output, _, _ = encoder_lstm1_layer(encoder_embedding_out) # We only need the sequence output for the next layer\n",
    "    encoder_lstm2_output, _, _ = encoder_lstm2_layer(encoder_lstm1_output)\n",
    "    encoder_lstm3_output, state_h3_enc_inf, state_c3_enc_inf = encoder_lstm3_layer(encoder_lstm2_output) # Get states from the last one\n",
    "\n",
    "    encoder_states_inf = [state_h3_enc_inf, state_c3_enc_inf]\n",
    "    encoder_model = tf.keras.Model(encoder_inputs_inf, encoder_states_inf, name='encoder_inference_model')\n",
    "\n",
    "    print(\"Encoder Inference Model set up.\")\n",
    "    # encoder_model.summary() # Optional summary\n",
    "\n",
    "    # --- Decoder Inference Model ---\n",
    "    # Takes the single previous word and the previous decoder states,\n",
    "    # outputs the next word probabilities and the new states.\n",
    "    decoder_single_word_input_inf = tf.keras.Input(shape=(1,), name='decoder_single_word_input_inf')\n",
    "    decoder_state_input_h_inf = tf.keras.Input(shape=(RNN_SIZE,), name='decoder_state_input_h_inf')\n",
    "    decoder_state_input_c_inf = tf.keras.Input(shape=(RNN_SIZE,), name='decoder_state_input_c_inf')\n",
    "    decoder_states_inputs_inf = [decoder_state_input_h_inf, decoder_state_input_c_inf]\n",
    "\n",
    "    decoder_single_word_embedding_inf = decoder_embedding_layer(decoder_single_word_input_inf)\n",
    "\n",
    "    # The decoder LSTM needs to run for a single timestep, taking previous states\n",
    "    # Using the original trained layer instance with a single timestep input shape handles this\n",
    "    decoder_outputs_inf, state_h_dec_inf, state_c_dec_inf = decoder_lstm_layer(\n",
    "        decoder_single_word_embedding_inf, initial_state=decoder_states_inputs_inf\n",
    "    )\n",
    "\n",
    "    # Apply the dense layer to the output of the single timestep\n",
    "    decoder_pred_output_inf = decoder_dense_layer(decoder_outputs_inf) # Shape (None, 1, VOCAB_SIZE)\n",
    "\n",
    "    # We need the probabilities for the single timestep, so slice the result\n",
    "    decoder_pred_output_inf = decoder_pred_output_inf[:, 0, :] # Shape (None, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "    decoder_model = tf.keras.Model(\n",
    "        [decoder_single_word_input_inf] + decoder_states_inputs_inf,\n",
    "        [decoder_pred_output_inf, state_h_dec_inf, state_c_dec_inf], # Output predicted probs and new states\n",
    "        name='decoder_inference_model'\n",
    "    )\n",
    "\n",
    "    print(\"Decoder Inference Model set up.\")\n",
    "    print(\"Inference models built by reusing trained layers.\")\n",
    "    # decoder_model.summary() # Optional summary\n",
    "\n",
    "except ValueError as e:\n",
    "     print(f\"Error setting up inference models. Could not get layer by name: {e}\")\n",
    "     print(\"Please check the layer names in your original training model summary and ensure they match the names used in this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8600d5d-8df4-4967-880a-6d71b07eb8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences_to_text helper function defined (with debug prints).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5 (Decoding Helper Function) - FINAL CORRECTED VERSION with Debug Prints ---\n",
    "\n",
    "def sequences_to_text(sequence, stop_on_end_token=True):\n",
    "    \"\"\"\n",
    "    Converts a sequence of word IDs back to a sentence.\n",
    "    Explicitly accesses necessary variables from the global scope.\n",
    "    Includes debug prints to show what's happening.\n",
    "    \"\"\"\n",
    "    # Explicitly check for necessary variables in the global scope\n",
    "    if 'reverse_word_index' not in globals():\n",
    "         print(\"Error [sequences_to_text]: 'reverse_word_index' is not defined in global scope. Please run testingMonograph.ipynb Cell 2.\")\n",
    "         return \"Decoding error: Missing reverse_word_index.\"\n",
    "    if 'OOV_TOKEN' not in globals():\n",
    "         print(\"Error [sequences_to_text]: 'OOV_TOKEN' is not defined in global scope. Please run testingMonograph.ipynb Cell 1.\")\n",
    "         return \"Decoding error: Missing OOV_TOKEN.\"\n",
    "    if 'START_TOKEN' not in globals():\n",
    "         print(\"Error [sequences_to_text]: 'START_TOKEN' is not defined in global scope. Please run testingMonograph.ipynb Cell 1.\")\n",
    "         return \"Decoding error: Missing START_TOKEN.\"\n",
    "    if 'END_TOKEN' not in globals():\n",
    "         print(\"Error [sequences_to_text]: 'END_TOKEN' is not defined in global scope. Please run testingMonograph.ipynb Cell 1.\")\n",
    "         return \"Decoding error: Missing END_TOKEN.\"\n",
    "\n",
    "    # Get references to the global variables once at the start of the function call\n",
    "    global reverse_word_index, OOV_TOKEN, START_TOKEN, END_TOKEN\n",
    "\n",
    "    # --- Debug Print ---\n",
    "    print(f\"--- sequences_to_text Debug ---\")\n",
    "    print(f\"Input sequence IDs: {sequence}\")\n",
    "    print(f\"Stop on END token: {stop_on_end_token}\")\n",
    "    print(f\"START_TOKEN: '{START_TOKEN}', END_TOKEN: '{END_TOKEN}', OOV_TOKEN: '{OOV_TOKEN}'\")\n",
    "    # --- End Debug Print ---\n",
    "\n",
    "Small vocabulary can lead to VOO only \n",
    "\n",
    "When the vocabulary is too big it may lead to OOM\n",
    "\n",
    "There is a limit to how much a model can be perfected  (convergence point)\n",
    "\n",
    "The time for epoch will grow constantly in a semi exponential way (means that the more epochs the longer it will start taking)\n",
    "\n",
    "The loss can be a not accurate measurement if based on the similarity\n",
    "    text = []\n",
    "    for word_id in sequence:\n",
    "        if word_id == 0: # Padding ID (assuming 0 is padding)\n",
    "            # --- Debug Print ---\n",
    "            # print(f\"  Skipping padding ID 0\")\n",
    "            # --- End Debug Print ---\n",
    "            continue\n",
    "\n",
    "        # Explicitly access the global reverse_word_index and OOV_TOKEN\n",
    "        word = reverse_word_index.get(word_id, OOV_TOKEN)\n",
    "\n",
    "        # Stop if the END token is predicted (and we are set to stop)\n",
    "        # Explicitly access the global END_TOKEN\n",
    "        if stop_on_end_token and word == END_TOKEN:\n",
    "            # --- Debug Print ---\n",
    "            print(f\"  Encountered END token ('{END_TOKEN}'). Stopping decoding.\")\n",
    "            # --- End Debug Print ---\n",
    "            break\n",
    "\n",
    "        # Avoid adding the START token itself to the output text\n",
    "        # Explicitly access the global START_TOKEN\n",
    "        if word != START_TOKEN:\n",
    "            # --- Debug Print ---\n",
    "            # print(f\"  Adding word ID {word_id} ('{word}')\")\n",
    "            # --- End Debug Print ---\n",
    "            text.append(word)\n",
    "        # else:\n",
    "            # --- Debug Print ---\n",
    "            # print(f\"  Skipping START token ('{START_TOKEN}')\")\n",
    "            # --- End Debug Print ---\n",
    "\n",
    "\n",
    "    # Join words and clean up leading/trailing whitespace\n",
    "    generated_text = \" \".join(text).strip()\n",
    "    # --- Debug Print ---\n",
    "    print(f\"Final generated text: '{generated_text}'\")\n",
    "    print(f\"--- sequences_to_text Debug End ---\")\n",
    "    # --- End Debug Print ---\n",
    "    return generated_text\n",
    "\n",
    "print(\"sequences_to_text helper function defined (with debug prints).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57fd772-6cd1-4148-8e0d-cb90fff4931e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_headline function defined (with simulation option and corrected syntax).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6 (Headline Generation Function) - Definition with debug prints ---\\n\",\n",
    "\n",
    "def generate_headline(input_text, max_headline_length=MAX_HEADLINE_LEN, simulate_output=False):\n",
    "    \"\"\"\n",
    "    Generates a headline for a given input article text using the loaded Seq2Seq models.\n",
    "    Includes an option to simulate output for testing the decoding pipeline without full training.\n",
    "    Includes debug prints to show prediction steps.\n",
    "\n",
    "    Args:\n",
    "        input_text: A string containing the article content.\n",
    "        max_headline_length: The maximum length of the generated headline sequence (including START token).\n",
    "        simulate_output: If True, bypasses model prediction and simulates a sequence of token IDs.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the generated headline.\n",
    "    \"\"\"\n",
    "    # --- DEBUG PRINTS START ---\n",
    "    print(\"\\n--- Debugging generate_headline ---\")\n",
    "    print(f\"'tokenizer' in globals(): {'tokenizer' in globals()}\")\n",
    "    print(f\"'encoder_model' in globals(): {'encoder_model' in globals()}\")\n",
    "    print(f\"'decoder_model' in globals(): {'decoder_model' in globals()}\")\n",
    "    print(f\"'START_TOKEN_ID' in globals(): {globals().get('START_TOKEN_ID') is not None}\") # Check if None\n",
    "    print(f\"'END_TOKEN_ID' in globals(): {globals().get('END_TOKEN_ID') is not None}\") # Check if None\n",
    "    print(f\"Simulation mode active: {simulate_output}\")\n",
    "    print(f\"MAX_HEADLINE_LEN: {max_headline_length}\")\n",
    "    # --- DEBUG PRINTS END ---\n",
    "\n",
    "    # Check for necessary components (less strict if simulating)\n",
    "    # Use .get() for START_TOKEN_ID and END_TOKEN_ID checks to avoid NameError if they weren't found by tokenizer\n",
    "    if 'tokenizer' not in globals() or ('encoder_model' not in globals() and not simulate_output) or ('decoder_model' not in globals() and not simulate_output) or globals().get('START_TOKEN_ID') is None or globals().get('END_TOKEN_ID') is None:\n",
    "         print(\"Error: Necessary components (tokenizer, inference models - unless simulating, token IDs) are not loaded or defined correctly.\")\n",
    "         return \"Generation failed: Model components or token IDs missing.\"\n",
    "\n",
    "    # Preprocess the input article text\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=MAX_ARTICLE_LEN, padding='post')\n",
    "\n",
    "    # Encode the input sequence to get the initial states for the decoder\n",
    "    try:\n",
    "        # Check if encoder_model is available or if we are simulating\n",
    "        if 'encoder_model' in globals() and encoder_model: # Added check for existence\n",
    "             # --- Debug Print ---\n",
    "             print(\"Running encoder_model.predict...\")\n",
    "             # --- End Debug Print ---\n",
    "             encoder_states = encoder_model.predict(input_seq, verbose=0)\n",
    "             # --- Debug Print ---\n",
    "             print(\"Encoder prediction complete.\")\n",
    "             # --- End Debug Print ---\n",
    "        elif simulate_output:\n",
    "             print(\"Encoder model not available, using dummy states in simulation.\")\n",
    "             # Provide dummy states if encoder fails in simulation\n",
    "             encoder_states = [np.zeros((1, RNN_SIZE)), np.zeros((1, RNN_SIZE))]\n",
    "        else:\n",
    "             print(\"Error: Encoder model not available and not in simulation mode.\")\n",
    "             return \"Generation failed: Encoder model missing.\"\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"Error during encoder_model.predict: {e}. encoder_model likely not available.\")\n",
    "        if not simulate_output:\n",
    "             return \"Generation failed: Encoder model predict error.\"\n",
    "        else:\n",
    "             print(\"Ignoring encoder predict error in simulation mode.\")\n",
    "             encoder_states = [np.zeros((1, RNN_SIZE)), np.zeros((1, RNN_SIZE))] # Dummy fallback\n",
    "    # REMOVED THE SYNTAX ERROR HERE: removed \\n\",\n",
    "    except Exception as e:\n",
    "         print(f\"Unexpected error during encoder_model.predict: {e}\")\n",
    "         if not simulate_output:\n",
    "              return \"Generation failed: Encoder model predict error.\"\n",
    "         else:\n",
    "              print(\"Ignoring encoder predict error in simulation mode.\")\n",
    "              encoder_states = [np.zeros((1, RNN_SIZE)), np.zeros((1, RNN_SIZE))] # Dummy fallback\n",
    "\n",
    "\n",
    "    # Initialize the decoder input with the START token ID\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = START_TOKEN_ID\n",
    "\n",
    "    # List to store the IDs of the generated headline sequence\n",
    "    generated_sequence_ids = []\n",
    "\n",
    "    # --- Simulation Setup ---\n",
    "    if simulate_output:\n",
    "        if 'reverse_word_index' not in globals():\n",
    "             print(\"Error: reverse_word_index not available for simulation.\")\n",
    "             return \"Simulation failed: reverse_word_index missing.\"\n",
    "        if 'OOV_TOKEN' not in globals():\n",
    "             print(\"Error: OOV_TOKEN not available for simulation.\")\n",
    "             return \"Simulation failed: OOV_TOKEN missing.\"\n",
    "\n",
    "        OOV_TOKEN_ID = tokenizer.word_index.get(globals()['OOV_TOKEN'], 1)\n",
    "\n",
    "        # Create a sample list of words. Get their IDs, or use OOV_TOKEN_ID if word not in vocab\n",
    "        sample_words = [\"this\", \"is\", \"a\", \"simulated\", \"headline\", \"for\", \"testing\", \"this\", \"decoding\", \"pipeline\"]\n",
    "        simulated_ids = [tokenizer.word_index.get(word.lower(), OOV_TOKEN_ID) for word in sample_words]\n",
    "        simulated_ids.append(END_TOKEN_ID)\n",
    "\n",
    "        print(f\"Simulating prediction with IDs: {simulated_ids}\")\n",
    "        sim_idx = 0\n",
    "\n",
    "    # Loop to predict the next word token by token\n",
    "    print(\"Starting decoding loop...\") # Added print\n",
    "    for i in range(max_headline_length):\n",
    "        # --- Debug Print ---\n",
    "        # print(f\"Decoding step {i+1}/{max_headline_length}\")\n",
    "        # --- End Debug Print ---\n",
    "\n",
    "        if simulate_output:\n",
    "            # In simulation mode, get the next ID from the predefined list\n",
    "            if sim_idx < len(simulated_ids):\n",
    "                predicted_token_id = simulated_ids[sim_idx]\n",
    "                sim_idx += 1\n",
    "            else:\n",
    "                # If simulation list is exhausted, force prediction of END token to stop\n",
    "                predicted_token_id = END_TOKEN_ID\n",
    "\n",
    "            # Dummy state update in simulation (not strictly necessary for this simulation type)\n",
    "            h, c = encoder_states # Just keep states constant\n",
    "\n",
    "            # --- Debug Print ---\n",
    "            predicted_word = globals().get('reverse_word_index', {}).get(predicted_token_id, globals().get('OOV_TOKEN', '<Unknown>'))\n",
    "            print(f\"Sim Step {i}: Predicted ID {predicted_token_id} ({predicted_word})\")\n",
    "            # --- End Debug Print ---\n",
    "\n",
    "\n",
    "        else: # Standard inference mode\n",
    "            # Check if decoder_model is available\n",
    "            if 'decoder_model' not in globals() or not decoder_model: # Added check for existence\n",
    "                 print(f\"Error during decoder_model.predict step {i}: decoder_model is not available.\")\n",
    "                 return \"Generation failed: Decoder model missing.\"\n",
    "            try:\n",
    "                # --- Debug Print ---\n",
    "                # print(f\"  Running decoder_model.predict with target_seq shape {target_seq.shape} and encoder_states...\")\n",
    "                # --- End Debug Print ---\n",
    "                output_tokens, h, c = decoder_model.predict(\n",
    "                    [target_seq] + encoder_states, verbose=0\n",
    "                )\n",
    "                # --- Debug Print ---\n",
    "                # print(f\"  Decoder prediction output_tokens shape: {output_tokens.shape}\")\n",
    "                # print(f\"  Decoder predicted states h shape: {h.shape}, c shape: {c.shape}\")\n",
    "                # --- End Debug Print ---\n",
    "\n",
    "            # REMOVED THE SYNTAX ERROR HERE: removed \\n\",\n",
    "            except Exception as e:\n",
    "                 print(f\"Unexpected error during decoder_model.predict step {i}: {e}\")\n",
    "                 return \"Generation failed: Decoder model predict error.\\n\"\n",
    "\n",
    "\n",
    "            # Sample the next token ID (Greedy search: pick the token with the highest probability)\n",
    "            predicted_token_id = np.argmax(output_tokens[0, :])\n",
    "\n",
    "            # --- Debug Print ---\n",
    "            predicted_word = globals().get('reverse_word_index', {}).get(predicted_token_id, globals().get('OOV_TOKEN', '<Unknown>'))\n",
    "            print(f\"Step {i}: Predicted ID {predicted_token_id} ({predicted_word})\")\n",
    "            # --- End Debug Print ---\n",
    "\n",
    "\n",
    "        # Append the predicted token ID to the generated sequence\n",
    "        generated_sequence_ids.append(predicted_token_id)\n",
    "\n",
    "        # Check if the predicted token is the END token\n",
    "        # Add a check before using END_TOKEN_ID\n",
    "        if END_TOKEN_ID is not None and predicted_token_id == END_TOKEN_ID:\n",
    "            print(\"END token predicted. Stopping generation.\") # Added print\n",
    "            break # Stop decoding\n",
    "\n",
    "        # Update the target sequence for the next step (only needed in standard mode)\n",
    "        if not simulate_output:\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = predicted_token_id\n",
    "\n",
    "            # Update the decoder states for the next step (only needed in standard mode)\n",
    "            encoder_states = [h, c] # These are actually the decoder states now\n",
    "\n",
    "\n",
    "    # Convert the sequence of IDs to text\n",
    "    # Ensure decoding components are available before calling sequences_to_text\n",
    "    # The checks are also inside sequences_to_text, but belt and suspenders\n",
    "    if 'reverse_word_index' not in globals() or 'OOV_TOKEN' not in globals() or 'START_TOKEN' not in globals() or 'END_TOKEN' not in globals():\n",
    "         print(\"Error: Decoding components (reverse_word_index, OOV_TOKEN, tokens) are not defined.\")\n",
    "         return \"Generation failed: Decoding components missing.\"\n",
    "\n",
    "    print(\"Decoding sequence IDs to text...\") # Added print\n",
    "    generated_headline_text = sequences_to_text(generated_sequence_ids, stop_on_end_token=True)\n",
    "\n",
    "    print(\"Decoding complete.\") # Added print\n",
    "    print(\"--- Debugging generate_headline End ---\\n\") # Added print\n",
    "\n",
    "    return generated_headline_text\n",
    "\n",
    "print(\"generate_headline function defined (with simulation option and corrected syntax).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcb2174-9276-471f-861b-60c8487225d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data from 'data/title_content_pair.pkl'...\n",
      "Loaded 50000 records.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 7 (Load Original Data for Testing) ---\n",
    "data_filepath = 'data/title_content_pair.pkl' # Path to your original data file\n",
    "\n",
    "if not os.path.exists(data_filepath):\n",
    "    print(f\"Error: Data file not found at '{data_filepath}'.\")\n",
    "    print(\"Cannot load original data for testing.\")\n",
    "    # You'll need to provide input_text manually if this fails\n",
    "else:\n",
    "    print(f\"Loading original data from '{data_filepath}'...\")\n",
    "    with open(data_filepath, 'rb') as fp:\n",
    "        loaded_data = pickle.load(fp)\n",
    "\n",
    "    # Extract titles and contents (titles include <start>/<end> if you modified Cell 2)\n",
    "    title_list = [item.get('title') for item in loaded_data]\n",
    "    content_list = [item.get('content') for item in loaded_data]\n",
    "    print(f\"Loaded {len(loaded_data)} records.\\n\") # Added newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba70c1df-e389-4b95-b301-b50dfb8b2828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating headline for article index 15...\n",
      "\n",
      "--- Debugging generate_headline ---\n",
      "'tokenizer' in globals(): True\n",
      "'encoder_model' in globals(): True\n",
      "'decoder_model' in globals(): True\n",
      "'START_TOKEN_ID' in globals(): True\n",
      "'END_TOKEN_ID' in globals(): True\n",
      "Simulation mode active: False\n",
      "MAX_HEADLINE_LEN: 50\n",
      "Running encoder_model.predict...\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d06a8a32f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d06a8a32f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Encoder prediction complete.\n",
      "Starting decoding loop...\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d068c97e560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7d068c97e560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Step 0: Predicted ID 36 (<start>)\n",
      "Step 1: Predicted ID 36 (<start>)\n",
      "Step 2: Predicted ID 1 (<OOV>)\n",
      "Step 3: Predicted ID 1 (<OOV>)\n",
      "Step 4: Predicted ID 1 (<OOV>)\n",
      "Step 5: Predicted ID 1 (<OOV>)\n",
      "Step 6: Predicted ID 1 (<OOV>)\n",
      "Step 7: Predicted ID 1 (<OOV>)\n",
      "Step 8: Predicted ID 1 (<OOV>)\n",
      "Step 9: Predicted ID 1 (<OOV>)\n",
      "Step 10: Predicted ID 1 (<OOV>)\n",
      "Step 11: Predicted ID 1 (<OOV>)\n",
      "Step 12: Predicted ID 1 (<OOV>)\n",
      "Step 13: Predicted ID 1 (<OOV>)\n",
      "Step 14: Predicted ID 1 (<OOV>)\n",
      "Step 15: Predicted ID 1 (<OOV>)\n",
      "Step 16: Predicted ID 1 (<OOV>)\n",
      "Step 17: Predicted ID 1 (<OOV>)\n",
      "Step 18: Predicted ID 1 (<OOV>)\n",
      "Step 19: Predicted ID 1 (<OOV>)\n",
      "Step 20: Predicted ID 1 (<OOV>)\n",
      "Step 21: Predicted ID 1 (<OOV>)\n",
      "Step 22: Predicted ID 1 (<OOV>)\n",
      "Step 23: Predicted ID 1 (<OOV>)\n",
      "Step 24: Predicted ID 1 (<OOV>)\n",
      "Step 25: Predicted ID 1 (<OOV>)\n",
      "Step 26: Predicted ID 1 (<OOV>)\n",
      "Step 27: Predicted ID 1 (<OOV>)\n",
      "Step 28: Predicted ID 1 (<OOV>)\n",
      "Step 29: Predicted ID 1 (<OOV>)\n",
      "Step 30: Predicted ID 1 (<OOV>)\n",
      "Step 31: Predicted ID 1 (<OOV>)\n",
      "Step 32: Predicted ID 1 (<OOV>)\n",
      "Step 33: Predicted ID 1 (<OOV>)\n",
      "Step 34: Predicted ID 1 (<OOV>)\n",
      "Step 35: Predicted ID 1 (<OOV>)\n",
      "Step 36: Predicted ID 1 (<OOV>)\n",
      "Step 37: Predicted ID 1 (<OOV>)\n",
      "Step 38: Predicted ID 1 (<OOV>)\n",
      "Step 39: Predicted ID 1 (<OOV>)\n",
      "Step 40: Predicted ID 1 (<OOV>)\n",
      "Step 41: Predicted ID 1 (<OOV>)\n",
      "Step 42: Predicted ID 1 (<OOV>)\n",
      "Step 43: Predicted ID 1 (<OOV>)\n",
      "Step 44: Predicted ID 1 (<OOV>)\n",
      "Step 45: Predicted ID 1 (<OOV>)\n",
      "Step 46: Predicted ID 1 (<OOV>)\n",
      "Step 47: Predicted ID 1 (<OOV>)\n",
      "Step 48: Predicted ID 1 (<OOV>)\n",
      "Step 49: Predicted ID 1 (<OOV>)\n",
      "Decoding sequence IDs to text...\n",
      "--- sequences_to_text Debug ---\n",
      "Input sequence IDs: [36, 36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Stop on END token: True\n",
      "START_TOKEN: '<start>', END_TOKEN: '<end>', OOV_TOKEN: '<OOV>'\n",
      "Final generated text: '<OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>'\n",
      "--- sequences_to_text Debug End ---\n",
      "Decoding complete.\n",
      "--- Debugging generate_headline End ---\n",
      "\n",
      "\n",
      "--- Original Article (First 500 chars) ---\n",
      " Daily Mail  Sunday 27th September, 2015  \n",
      " 'We're too broke to go on the beat': Police chiefs warn ministers that cuts mean they can't do their job as they threaten to stop street patrols and say they won't be able to protect public from rioters or terrorists?... \n",
      "Read the full story at Daily Mail...\n",
      "\n",
      "--- Original Headline ---\n",
      " <start> Police chiefs warn ministers Were too broke to go on the beat <end>\n",
      "\n",
      "--- Generated Headline (Simulated) ---\n",
      " <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "\n",
      "--- Comparison (Simulated Output) ---\n",
      "Levenshtein Distance: 276\n",
      "Normalized Similarity (higher is better): 0.0383\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8 (Generate and Display Headline) ---\n",
    "\n",
    "if 'content_list' not in globals() or not content_list: # Check globals() as data loading might be in a separate cell\n",
    "    print(\"\\nError: 'content_list' is not loaded. Please run Cell 7 (Load Original Data) or provide input_text manually.\")\n",
    "else:\n",
    "    # Choose an article index to generate a headline for\n",
    "    article_index_to_test = 15 # You can change this index\n",
    "\n",
    "    if article_index_to_test < 0 or article_index_to_test >= len(content_list):\n",
    "        print(f\"Error: Article index {article_index_to_test} is out of bounds.\")\n",
    "    else:\n",
    "        input_article_text = content_list[article_index_to_test]\n",
    "        original_headline = title_list[article_index_to_test] # Includes <start>/<end> if modified in Cell 2\n",
    "\n",
    "        print(f\"\\nGenerating headline for article index {article_index_to_test}...\")\n",
    "\n",
    "        # Generate the headline - Use simulate_output=True for testing the pipeline\n",
    "        # Use simulate_output=False later when you have a sufficiently trained model\n",
    "        # Ensure generate_headline function is defined in a cell *before* this one\n",
    "        if 'generate_headline' in globals():\n",
    "             generated_headline = generate_headline(input_article_text, simulate_output=False) # <--- Call with parameter\n",
    "        else:\n",
    "             print(\"Error: 'generate_headline' function is not defined. Please run the cell containing its definition.\")\n",
    "             generated_headline = \"Generation failed: Function not defined.\"\n",
    "\n",
    "\n",
    "        print(\"\\n--- Original Article (First 500 chars) ---\\n\", input_article_text[:500] + '...') # Added newline\n",
    "        print(\"\\n--- Original Headline ---\\n\", original_headline) # Added newline\n",
    "        print(\"\\n--- Generated Headline (Simulated) ---\\n\", generated_headline) # Changed label & added newline\n",
    "\n",
    "\n",
    "        # Optional: Basic comparison using Levenshtein Distance (will compare simulated output)\n",
    "        # This comparison is less meaningful with simulated output but tests the metric calculation\n",
    "        # This block was previously misplaced and had syntax errors\n",
    "        if 'Levenshtein' in globals() and original_headline and generated_headline and \"Generation failed\" not in generated_headline: # Check if Levenshtein is imported and generation wasn't an error\n",
    "             # Clean original headline for comparison (remove <start>/<end>)\n",
    "             # Ensure START_TOKEN and END_TOKEN are defined globally (from Cell 1)\n",
    "             if 'START_TOKEN' in globals() and 'END_TOKEN' in globals():\n",
    "                 cleaned_original_headline = original_headline.replace(START_TOKEN, '').replace(END_TOKEN, '').strip()\n",
    "             else:\n",
    "                 print(\"Warning: START_TOKEN or END_TOKEN not defined, cannot clean original headline for comparison.\")\n",
    "                 cleaned_original_headline = original_headline # Use original as is if cleaning tokens not found\n",
    "\n",
    "             if cleaned_original_headline: # Avoid dividing by zero if original is empty after cleaning\n",
    "                 ld = Levenshtein.distance(cleaned_original_headline, generated_headline)\n",
    "                 max_len = max(len(cleaned_original_headline), len(generated_headline))\n",
    "                 similarity_score = (max_len - ld) / max_len if max_len > 0 else 0\n",
    "                 print(f\"\\n--- Comparison (Simulated Output) ---\")\n",
    "                 # CORRECTED PRINT LINES - REMOVED THE TRAILING BACKSLASHES INSIDE THE STRING\n",
    "                 print(f\"Levenshtein Distance: {ld}\")\n",
    "                 print(f\"Normalized Similarity (higher is better): {similarity_score:.4f}\")\n",
    "             else:\n",
    "                 print(\"\\n--- Comparison (Simulated Output) ---\\nOriginal headline is empty after cleaning.\") # Added newline\n",
    "        elif \"Generation failed\" in generated_headline:\n",
    "             print(\"\\nSkipping comparison due to generation failure.\")\n",
    "        else:\n",
    "             print(\"\\nSkipping comparison: Levenshtein not imported, or headlines/original empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f886003-4477-427e-a1cd-4db4005a4e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b29108-eb30-4319-a7a7-f7308f97faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced1a11-5f4a-4372-ad5a-761e1b91b3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6419f-5bc5-4de8-b2ab-a4e50adae9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296a8f1-6dfd-4eea-aa61-a832459b6369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd9447-ce9c-4bff-a131-2d4c6f01e23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
